# Awesome-Embodied-Agent-for-Long-Horizon-Tasks-Latest-Survey

Start your journey in embodied agent for long-horizon tasks. Keep updated.

##  üöÄ Survey

The list is designed to help you build a solid worldview of the field, guiding you from foundational concepts to advanced topics, and from broad perspectives to specific technical challenges. <font color=yellow>Please read in order.</font>

##  üöÄ Talks or Tutorials

- Embodied-Agent-for-Long-Horizon-Tasks Talks or Tutorials

##  üöÄ Workshops or Challenges

- Embodied-Agent-for-Long-Horizon-Tasks Workshops or Challenges

##  üó∫Ô∏è Simulator

- Embodied-Agent-for-Long-Horizon-Tasks Simulator

##  üó∫Ô∏è Robotics

- Embodied-Agent-for-Long-Horizon-Tasks Robotics

##  üìö Papers

### 1„ÄÅService Embodied Agents for Long-Horizon Tasks

---
- Service Embodied Agents for Long-Horizon Tasks

### 2„ÄÅIndustrial Embodied Agents for Long-Horizon Tasks

---
- Industrial Embodied Agents for Long-Horizon Tasks

### 3„ÄÅDriving Embodied Agents for Long-Horizon Tasks

---
- Driving Embodied Agents for Long-Horizon Tasks

### 4„ÄÅEntertainment Embodied Agents for Long-Horizon Tasks

---
- Entertainment Embodied Agents for Long-Horizon Tasks

####  __`CVPR 2025`__
- Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy; [Paper](https://arxiv.org/pdf/2502.19902), [Paper Code](https://github.com/dawn0815/Optimus-2) , __`‚ú® 8`__

####  __`NeurIPS 2024`__
- Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents; [Paper](https://arxiv.org/pdf/2302.01560), [Paper Code](https://github.com/CraftJarvis/MC-Planner) , __`‚ú® 270`__
- Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks; [Paper](https://openreview.net/pdf?id=XXOMCwZ6by), [Paper Code](https://github.com/JiuTian-VL/Optimus-1) , __`‚ú® 70`__

####  __`ICLR 2024`__
- Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds; [Paper](https://arxiv.org/pdf/2310.13255), [Paper Code](https://github.com/BAAI-Agents/Steve-Eye) , __`‚ú® 7`__

####  __`NeurIPS 2023`__
- Active Reasoning in an Open-World Environment; [Paper](https://arxiv.org/abs/2311.02018), [Paper Code](https://github.com/ariesssxu/Conan-Active-Reasoning) , __`‚ú® `__


**Êñ∞Â¢ûÔºö**
####  __`CVPR 2019`__
- Scene Memory Transformer for Embodied Agents in Long-Horizon Tasks; [Paper](https://arxiv.org/abs/1903.03878), __`‚ú® `__

####  __`CVPR 2024`__
- SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World; [Paper](https://cvpr.thecvf.com/virtual/2024/poster/30192), [Paper Code](https://github.com/allenai/spoc-robot-training), __`‚ú® 117`__
- Generate Subgoal Images before Act: Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts; [Paper](https://cotdiffusion.github.io/), __`‚ú® `__
- Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld; [Paper](https://cvpr.thecvf.com/virtual/2024/poster/30385), [Paper Code](https://github.com/stevenyangyj/Emma-Alfworld), __`‚ú® 55`__

####  __`CVPR 2023`__
- EXCALIBUR: Encouraging and Evaluating Embodied Exploration; [Paper](https://cvpr.thecvf.com/virtual/2023/poster/21602), __`‚ú® `__
- Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction; [Paper](https://cvpr.thecvf.com/virtual/2023/poster/23204), [Paper Code](https://github.com/CraftJarvis/MC-Controller), __`‚ú® 46`__

####  __`IEEE RA-L 2023`__
- SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World; [Paper](https://cvpr.thecvf.com/virtual/2024/poster/30192), [Paper Code](https://github.com/RobotLL/ERRA), __`‚ú® 7`__

####  __`ICRA 2024`__
- RoboVQA: Multimodal Long-Horizon Reasoning for Robotics; [Paper](https://arxiv.org/abs/2311.00899), [Paper Code](https://robovqa.github.io/), __`‚ú® 24`__

####  __`NeurIPS 2024`__
- Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making; [Paper](https://arxiv.org/abs/2410.07166), [Paper Code](https://github.com/embodied-agent-interface/embodied-agent-interface) , __`‚ú® 190`__
- Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments; [Paper](https://arxiv.org/abs/2407.10031), [Paper Code](https://github.com/nsidn98/LLaMAR) , __`‚ú® 9`__

####  __`ICLR 2025`__
- Memory-Driven Multimodal Chain of Thought for Embodied Long-Horizon Task Planning; [Paper](https://openreview.net/forum?id=Z1Va3Ue4GF), , __`‚ú® `__



##  üéØ Announcement

Besides the wonderful papers we list above, we are very happy to announce that our group, THU-AEAI (Autonomous Embodied AI) Laboratory, recently released a preprint titled: [EvoAgent: Agent Autonomous Evolution with Continual World Model for Long-Horizon Tasks](https://arxiv.org/pdf/2502.05907), the first autonomous-evolution agent with a continual World Model, which can autonomously complete various long-horizon tasks across environments through self-planning, self-control, and self-reflection, without human intervention. If this paper inspires you, you may consider cite it via:
```bibtex
@article{feng2025evoagent,
  title={EvoAgent: Agent Autonomous Evolution with Continual World Model for Long-Horizon Tasks},
  author={Tongtong Feng and Xin Wang and Zekai Zhou and Ren Wang and Yuwei Zhan and Guangyao Li and Qing Li and Wenwu Zhu},
  journal={arXiv preprint arXiv:2502.05907},
  year={2025}
}
```


## Other Related Repos
- [Awesome-Minecraft](https://github.com/Incendo/awesome-minecraft)
- [Awesome-Minecraft-Agent](https://github.com/dawn0815/Awesome-Minecraft-Agent)

    